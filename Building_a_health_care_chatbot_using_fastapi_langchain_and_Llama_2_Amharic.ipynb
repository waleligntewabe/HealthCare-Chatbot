{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsRKw/V4nBf/aIfcKp3CD9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waleligntewabe/HealthCare-Chatbot/blob/main/Building_a_health_care_chatbot_using_fastapi_langchain_and_Llama_2_Amharic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build a Health care chatbot"
      ],
      "metadata": {
        "id": "Em7thokTmHat"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building a healthcare chatbot using FastAPI, LangChain, and Llama 2 Amharic sounds like an exciting project! FastAPI is great for building APIs quickly, LangChain provides language translation capabilities, and Llama 2 Amharic can help with conversational interfaces in Amharic.\n",
        "\n",
        "Here's a high-level overview of how you might approach this project:\n",
        "\n",
        "Set Up FastAPI: Start by setting up a FastAPI project. FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.7+.\n",
        "\n",
        "Integrate LangChain: Integrate LangChain into your FastAPI project. LangChain is a library or service that provides language translation capabilities. You'll use this to translate user queries from Amharic to a language your chatbot understands and vice versa.\n",
        "\n",
        "Implement Chatbot Logic: Design and implement the logic for your healthcare chatbot. This involves understanding user queries, processing them, and providing appropriate responses. You might use natural language processing (NLP) techniques for understanding user intents and responding accordingly.\n",
        "\n",
        "Integrate Llama 2 Amharic: Incorporate Llama 2 Amharic into your chatbot to enable it to understand and respond in Amharic. This may involve training the chatbot with data in Amharic and fine-tuning its responses to be culturally appropriate.\n",
        "\n",
        "Test and Iterate: Test your chatbot extensively to ensure it provides accurate and helpful responses. Iterate on both the functionality and the language capabilities to improve the user experience.\n",
        "\n",
        "Deploy: Once you're satisfied with your chatbot, deploy it so users can interact with it. FastAPI makes it easy to deploy your API, and you can host it on various platforms such as AWS, Azure, or Google Cloud Platform."
      ],
      "metadata": {
        "id": "7fSqcLRdmO8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A\n",
        "### 1. Install FastAPI and Uvicorn:"
      ],
      "metadata": {
        "id": "LG-oG9o5oB6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Z4rUbrMmNvy",
        "outputId": "016121b4-1b1e-46de-bb22-a8215d575042"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.109.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m922.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn\n",
            "  Downloading uvicorn-0.27.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (1.10.14)\n",
            "Collecting starlette<0.36.0,>=0.35.0 (from fastapi)\n",
            "  Downloading starlette-0.35.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.8.0 (from fastapi)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.36.0,>=0.35.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.36.0,>=0.35.0->fastapi) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.36.0,>=0.35.0->fastapi) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.36.0,>=0.35.0->fastapi) (1.2.0)\n",
            "Installing collected packages: typing-extensions, h11, uvicorn, starlette, fastapi\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fastapi-0.109.0 h11-0.14.0 starlette-0.35.1 typing-extensions-4.9.0 uvicorn-0.27.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Create a FastAPI App:\n",
        "Next, let's create a FastAPI application. Create a new Python file (let's name it main.py) and define a simple FastAPI app."
      ],
      "metadata": {
        "id": "740YV1dooMvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# main.py\n",
        "from fastapi import FastAPI\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def read_root():\n",
        "    return {\"message\": \"Hello, World\"}\n"
      ],
      "metadata": {
        "id": "DYMdacW6oJbE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Run the FastAPI App:\n",
        "We'll use Uvicorn to run our FastAPI application. This command will start the server locally on port 8000."
      ],
      "metadata": {
        "id": "1DpGWPMMoXkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!uvicorn main:app --reload"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zhU-239oUd8",
        "outputId": "90c03d08-36fe-4f30-fb98-eade01075296"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mINFO\u001b[0m:     Will watch for changes in these directories: ['/content']\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://127.0.0.1:8000\u001b[0m (Press CTRL+C to quit)\n",
            "\u001b[32mINFO\u001b[0m:     Started reloader process [\u001b[36m\u001b[1m1418\u001b[0m] using \u001b[36m\u001b[1mStatReload\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m1420\u001b[0m]\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "\u001b[32mINFO\u001b[0m:     Shutting down\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application shutdown.\n",
            "\u001b[32mINFO\u001b[0m:     Application shutdown complete.\n",
            "\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m1420\u001b[0m]\n",
            "\u001b[32mINFO\u001b[0m:     Stopping reloader process [\u001b[36m\u001b[1m1418\u001b[0m]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qs2-Z0YfpySx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sQ7bsTeQpx4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B. Integrate LangChain"
      ],
      "metadata": {
        "id": "1cKM8vhUqv99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BUuZnQwq7QO",
        "outputId": "67159aec-227b-48d7-9981-aae0dff86742"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.24)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.14 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.16)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.16 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.16)\n",
            "Requirement already satisfied: langsmith<0.1,>=0.0.83 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.83)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.14)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain) (1.2.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/path/to/langchain')\n",
        "from langchain import LangChain\n"
      ],
      "metadata": {
        "id": "Af_c4MhwsBiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import LangChain"
      ],
      "metadata": {
        "id": "NUOFBM97tF9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import LangChain:\n",
        "In your FastAPI main.py file, import LangChain and any necessary modules:"
      ],
      "metadata": {
        "id": "WbvQEEBVrLhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# main.py\n",
        "from fastapi import FastAPI\n",
        "from langchain import LangChain\n",
        "\n",
        "app = FastAPI()\n",
        "lang_chain = LangChain()\n"
      ],
      "metadata": {
        "id": "nQijpeLrrD32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Translation Endpoint:\n",
        "\n",
        "Define an endpoint in your FastAPI app to handle translation requests. This endpoint will take input text and target language as parameters, translate the text, and return the translated text."
      ],
      "metadata": {
        "id": "mrpz3g-2rasg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "class TranslationRequest(BaseModel):\n",
        "    text: str\n",
        "    target_language: str\n",
        "\n",
        "@app.post(\"/translate/\")\n",
        "async def translate_text(request: TranslationRequest):\n",
        "    translated_text = lang_chain.translate(request.text, request.target_language)\n",
        "    return {\"translated_text\": translated_text}\n"
      ],
      "metadata": {
        "id": "lcCEp83JrXzf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the FastAPI App:\n",
        "Run your FastAPI app using Uvicorn:"
      ],
      "metadata": {
        "id": "97_4HdNlrnih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!uvicorn main:app --reload"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqGk-qearj6g",
        "outputId": "03346d65-bc71-4f5c-acc8-5799c0bab145"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mINFO\u001b[0m:     Will watch for changes in these directories: ['/content']\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://127.0.0.1:8000\u001b[0m (Press CTRL+C to quit)\n",
            "\u001b[32mINFO\u001b[0m:     Started reloader process [\u001b[36m\u001b[1m4389\u001b[0m] using \u001b[36m\u001b[1mStatReload\u001b[0m\n",
            "Process SpawnProcess-1:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/_subprocess.py\", line 78, in subprocess_started\n",
            "    target(sockets=sockets)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/server.py\", line 62, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "  File \"/usr/lib/python3.10/asyncio/runners.py\", line 44, in run\n",
            "    return loop.run_until_complete(main)\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\n",
            "    return future.result()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/server.py\", line 69, in serve\n",
            "    config.load()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/config.py\", line 458, in load\n",
            "    self.loaded_app = import_from_string(self.app)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/importer.py\", line 21, in import_from_string\n",
            "    module = importlib.import_module(module_str)\n",
            "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/content/main.py\", line 12, in <module>\n",
            "    from langchain import LangChain\n",
            "ImportError: cannot import name 'LangChain' from 'langchain' (/usr/local/lib/python3.10/dist-packages/langchain/__init__.py)\n",
            "\u001b[32mINFO\u001b[0m:     Stopping reloader process [\u001b[36m\u001b[1m4389\u001b[0m]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement Chatbot Logic:"
      ],
      "metadata": {
        "id": "YQNXelQkssy3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Design and implement the logic for your healthcare chatbot. This involves understanding user queries, processing them, and providing appropriate responses. You might use natural language processing (NLP) techniques for understanding user intents and responding accordingly."
      ],
      "metadata": {
        "id": "kjl4NyDgs1OF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing the chatbot logic involves several steps, including understanding user queries, processing them, and generating appropriate responses. Here's a general outline of how you can proceed:\n",
        "\n",
        "Define Intent and Entity Recognition: Identify the intents and entities relevant to healthcare queries. Intents represent the purpose or goal of a user's message (e.g., scheduling an appointment, asking about symptoms), while entities are specific pieces of information within the message (e.g., date, time, symptoms).\n",
        "\n",
        "Choose a Natural Language Processing (NLP) Framework: Select an NLP framework that supports intent recognition and entity extraction. Popular choices include spaCy, NLTK, and Rasa NLU.\n",
        "\n",
        "Train the NLP Model: Train the NLP model using labeled data to recognize intents and extract entities from user queries. You'll need annotated examples of user queries paired with their corresponding intents and entities.\n",
        "\n",
        "Implement Response Generation: Based on the recognized intent and extracted entities, define the logic for generating appropriate responses. This might involve querying a database, calling external APIs, or executing predefined workflows.\n",
        "\n",
        "Handle Dialog Management: Implement dialog management to maintain context across multiple interactions with the user. Keep track of the conversation history and guide the user through multi-turn interactions if necessary.\n",
        "\n",
        "Test and Iterate: Test the chatbot extensively to ensure accurate intent recognition, entity extraction, and response generation. Collect user feedback and iteratively improve the chatbot's performance based on the feedback received.\n",
        "\n",
        "Here's a simplified example demonstrating how you might implement a basic healthcare chatbot using FastAPI and a simple rule-based approach for intent recognition and response generation:"
      ],
      "metadata": {
        "id": "5UC43xOavLkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, HTTPException\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Define intents and their corresponding responses\n",
        "intents = {\n",
        "    \"greeting\": \"Hello! How can I assist you today?\",\n",
        "    \"appointment\": \"Sure, let's schedule an appointment. When would you like to come in?\",\n",
        "    \"symptoms\": \"Please describe your symptoms, and I'll do my best to help.\",\n",
        "    # Add more intents and responses as needed\n",
        "}\n",
        "\n",
        "# Endpoint to handle user queries\n",
        "@app.post(\"/query/\")\n",
        "async def process_query(query: str):\n",
        "    # Simplified rule-based approach for intent recognition\n",
        "    intent = get_intent(query)\n",
        "    if intent in intents:\n",
        "        return {\"response\": intents[intent]}\n",
        "    else:\n",
        "        raise HTTPException(status_code=404, detail=\"Intent not recognized\")\n",
        "\n",
        "def get_intent(query):\n",
        "    # Simplified rule-based intent recognition\n",
        "    if \"hello\" in query.lower():\n",
        "        return \"greeting\"\n",
        "    elif \"appointment\" in query.lower():\n",
        "        return \"appointment\"\n",
        "    elif \"symptoms\" in query.lower():\n",
        "        return \"symptoms\"\n",
        "    # Add more rules for intent recognition as needed\n",
        "    else:\n",
        "        return \"unknown\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "71DPwNtVsxq0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the FastAPI app\n",
        "if __name__ == \"__main__\":\n",
        "    import uvicorn\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ],
      "metadata": {
        "id": "RNHsms1LvRwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Integrate Llama 2 Amharic:"
      ],
      "metadata": {
        "id": "SlgeoGiNvwTe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Incorporate Llama 2 Amharic into your chatbot to enable it to understand and respond in Amharic. This may involve training the chatbot with data in Amharic and fine-tuning its responses to be culturally appropriate."
      ],
      "metadata": {
        "id": "b_QDgfNAwIIP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integrating Llama 2 Amharic into your chatbot involves several steps to enable it to understand and respond in Amharic. Here's how you can proceed:\n",
        "\n",
        "Install and Configure Llama 2 Amharic: Begin by installing the Llama 2 Amharic library and any necessary dependencies. Follow the installation instructions provided by the library's documentation. Additionally, configure Llama 2 Amharic to work within your FastAPI environment.\n",
        "\n",
        "Preprocess Amharic Data: If you have existing data in Amharic, preprocess it to prepare it for training the chatbot. This might involve tokenization, cleaning, and any other necessary preprocessing steps.\n",
        "\n",
        "Fine-tune the Chatbot Model: If you're using a pre-trained chatbot model, fine-tune it on Amharic data to adapt it to the language and domain of healthcare. This step helps the chatbot understand and generate responses in Amharic that are relevant to healthcare queries.\n",
        "\n",
        "Integrate Language Detection: Implement language detection functionality in your chatbot to determine the language of user queries. This allows the chatbot to switch between languages as needed and provide responses in the appropriate language.\n",
        "\n",
        "Translate Responses: Use Llama 2 Amharic to translate responses generated by the chatbot from the default language (e.g., English) to Amharic before sending them to the user. This ensures that the user receives responses in their preferred language.\n",
        "\n",
        "Test and Evaluate: Test the integrated chatbot thoroughly to ensure that it can understand user queries in Amharic, generate appropriate responses in Amharic, and seamlessly switch between languages. Collect feedback from users and iterate on the chatbot's performance as needed.\n",
        "\n",
        "Here's a simplified example demonstrating how you might integrate Llama 2 Amharic into your FastAPI chatbot:"
      ],
      "metadata": {
        "id": "Y0UpFkrkwPe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "from llama2amharic import Llama2Amharic\n",
        "\n",
        "app = FastAPI()\n",
        "llama = Llama2Amharic()\n",
        "\n",
        "# Endpoint to handle user queries\n",
        "@app.post(\"/query/\")\n",
        "async def process_query(query: str, lang: str = \"en\"):\n",
        "    if lang == \"am\":\n",
        "        # Translate the query to English for processing if it's in Amharic\n",
        "        query = llama.translate(query, source_lang=\"am\", target_lang=\"en\")\n",
        "    # Process the query and generate response in English\n",
        "    response_en = process_query_in_english(query)\n",
        "    if lang == \"am\":\n",
        "        # Translate the response to Amharic before returning it\n",
        "        response_am = llama.translate(response_en, source_lang=\"en\", target_lang=\"am\")\n",
        "        return {\"response\": response_am}\n",
        "    else:\n",
        "        return {\"response\": response_en}\n",
        "\n",
        "def process_query_in_english(query):\n",
        "    # Placeholder function to process query in English\n",
        "    # Replace this with your actual chatbot logic\n",
        "    return \"Placeholder response in English\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "K4DhwrI7vVkO",
        "outputId": "2c9ae1c8-74da-4b53-fe13-614a44b566fc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/tokenize.py:527: RuntimeWarning: coroutine 'Server.serve' was never awaited\n",
            "  pseudomatch = _compile(PseudoToken).match(line, pos)\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'llama2amharic'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-c27eeff53e3a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfastapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastAPI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mllama2amharic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLlama2Amharic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mapp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFastAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mllama\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLlama2Amharic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama2amharic'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the FastAPI app\n",
        "if __name__ == \"__main__\":\n",
        "    import uvicorn\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ],
      "metadata": {
        "id": "nq4OvUbRwUAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test and Iterate:"
      ],
      "metadata": {
        "id": "VaghywTywdlC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test your chatbot extensively to ensure it provides accurate and helpful responses. Iterate on both the functionality and the language capabilities to improve the user experience."
      ],
      "metadata": {
        "id": "W6WgiCaswkp6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S9zoY4-lwdSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test 1"
      ],
      "metadata": {
        "id": "f0787h1GmD5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwpUhUQTgPxD",
        "outputId": "e36a324b-198c-4f6a-d37d-cf1804dc0680"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/chatbot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58WniQPVgXmr",
        "outputId": "65847b4f-07be-4bd0-99be-37f69d182a4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/chatbot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bhRLWO82g6-",
        "outputId": "346390d2-4e40-4bbe-8c82-b25a3ad77aa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory '/content/drive/MyDrive/chatbot' already exists.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Specify the directory name\n",
        "directory = \"/content/drive/MyDrive/chatbot\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "    print(f\"Directory '{directory}' created successfully.\")\n",
        "else:\n",
        "    print(f\"Directory '{directory}' already exists.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd \"/content/drive/MyDrive/chatbot\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fj7Y9EVxeIXC",
        "outputId": "017054fc-8e0b-42ac-9f03-7f417d03dc49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/chatbot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m venv myenv\n",
        "!source myenv/bin/activate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1EC0FCG3zZg",
        "outputId": "c1bfabf4-3133-4e30-b25b-f385fa03bf43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The virtual environment was not created successfully because ensurepip is not\n",
            "available.  On Debian/Ubuntu systems, you need to install the python3-venv\n",
            "package using the following command.\n",
            "\n",
            "    apt install python3.10-venv\n",
            "\n",
            "You may need to use sudo with that command.  After installing the python3-venv\n",
            "package, recreate your virtual environment.\n",
            "\n",
            "Failing command: /content/drive/MyDrive/chatbot/myenv/bin/python3\n",
            "\n",
            "/bin/bash: line 1: myenv/bin/activate: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai fastapi langchain uvicorn"
      ],
      "metadata": {
        "id": "hhl67yfSg783"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the Chatbot API"
      ],
      "metadata": {
        "id": "aPPF0rP3hkQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use FastAPI, a modern web framework, to build the API. Open your favorite code editor and create a new file called main.py. Copy the following code into the file:"
      ],
      "metadata": {
        "id": "rL3pp2EGiDEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import uvicorn\n",
        "from fastapi import FastAPI, Request, Form\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import CSVLoader\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.vectorstores import DocArrayInMemorySearch\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Set your OpenAI API key here\n",
        "OPENAI_API_KEY = \"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\" #\"your_api_key\"\n",
        "\n",
        "def setup_chain():\n",
        "    # Define file path and template\n",
        "    file = 'Mental_Health_FAQ.csv'\n",
        "    template = \"\"\"\n",
        "        # Template contents here\n",
        "        \"\"\"\n",
        "\n",
        "    # Initialize embeddings, loader, and prompt\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "    loader = CSVLoader(file_path=file, encoding='utf-8')\n",
        "    docs = loader.load()\n",
        "    prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "    # Create DocArrayInMemorySearch and retriever\n",
        "    db = DocArrayInMemorySearch.from_documents(docs, embeddings)\n",
        "    retriever = db.as_retriever()\n",
        "    chain_type_kwargs = {\"prompt\": prompt}\n",
        "\n",
        "    # Initialize ChatOpenAI\n",
        "    llm = ChatOpenAI(\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    # Setup RetrievalQA chain\n",
        "    chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever,\n",
        "        chain_type_kwargs=chain_type_kwargs,\n",
        "        verbose=True\n",
        "    )\n",
        "    return chain\n"
      ],
      "metadata": {
        "id": "f9bq5gy0hnC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPKLhN19ll8s",
        "outputId": "a26ceb99-0c0d-49ce-ed34-f36636503318"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.0.5-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.16 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.1.16)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (1.23.5)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (1.10.0)\n",
            "Collecting tiktoken<0.6.0,>=0.5.2 (from langchain-openai)\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain-openai) (6.0.1)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1,>=0.0.83 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain-openai) (0.0.83)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain-openai) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain-openai) (1.10.14)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain-openai) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain-openai) (8.2.3)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (0.26.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<0.6.0,>=0.5.2->langchain-openai) (2023.6.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain-openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain-openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2,>=0.1.16->langchain-openai) (2.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-core<0.2,>=0.1.16->langchain-openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-core<0.2,>=0.1.16->langchain-openai) (2.0.7)\n",
            "Installing collected packages: tiktoken, langchain-openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-openai-0.0.5 tiktoken-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = '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'\n"
      ],
      "metadata": {
        "id": "lWfhhE6tlrWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(openai_api_key='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')\n"
      ],
      "metadata": {
        "id": "LqG_ssrlmAj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"langchain[docarray]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbeYmku7mGMs",
        "outputId": "5256b38e-f4be-43d2-ae17-2139448c27de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: -c: line 1: unexpected EOF while looking for matching `\"'\n",
            "/bin/bash: -c: line 2: syntax error: unexpected end of file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-multipart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHdMbO0gmVzW",
        "outputId": "11b4cb92-e2cc-4c3d-a7c9-42baee283222"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-multipart\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m41.0/45.7 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m968.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-multipart\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed python-multipart-0.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent = setup_chain()"
      ],
      "metadata": {
        "id": "JBB2xo1rmRDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nest_asyncio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJ3cB2uEmjn5",
        "outputId": "cd6f538f-a2f0-4822-f795-550a2879fbaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@app.get(\"/\")\n",
        "def read_root(request: Request):\n",
        "    return {\"message\": \"Welcome to the Mental Health Chatbot API!\"}\n",
        "\n",
        "@app.post(\"/prompt\")\n",
        "def process_prompt(prompt: str = Form(...)):\n",
        "    response = agent.run(prompt)\n",
        "    return {\"response\": response}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "BCuT8EKmiJl5",
        "outputId": "b7aa79a5-5289-49db-e1e6-02e34607ca29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "asyncio.run() cannot be called from a running event loop",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-1b09fc9d460d>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0muvicorn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"0.0.0.0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/uvicorn/main.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(app, host, port, uds, fd, loop, http, ws, ws_max_size, ws_max_queue, ws_ping_interval, ws_ping_timeout, ws_per_message_deflate, lifespan, interface, reload, reload_dirs, reload_includes, reload_excludes, reload_delay, workers, env_file, log_config, log_level, access_log, proxy_headers, server_header, date_header, forwarded_allow_ips, root_path, limit_concurrency, backlog, limit_max_requests, timeout_keep_alive, timeout_graceful_shutdown, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_version, ssl_cert_reqs, ssl_ca_certs, ssl_ciphers, headers, use_colors, app_dir, factory, h11_max_incomplete_event_size)\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0mMultiprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msockets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m         \u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muds\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pragma: py-win32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/uvicorn/server.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, sockets)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msockets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_event_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msockets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msockets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msockets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/runners.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \"\"\"\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m     34\u001b[0m             \"asyncio.run() cannot be called from a running event loop\")\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DW789jrrlGa6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}